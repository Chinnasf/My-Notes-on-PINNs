{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the [Lagrangian NN tutorial](https://colab.research.google.com/drive/1CSy-xfrnTX28p1difoTA8ulYw0zytJkq).\n",
    "\n",
    "## The double pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the creation of images\n",
    "#!pip install -U -q Pillow moviepy proglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.experimental.ode import odeint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial # reduces arguments to function by making some subset implicit\n",
    "\n",
    "from jax.example_libraries import stax, optimizers\n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from moviepy import ImageSequenceClip\n",
    "from functools import partial\n",
    "import proglog\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36103/2814231649.py:3: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
      "  print(xla_bridge.get_backend().platform)\n",
      "ERROR:2025-07-08 21:25:14,824:jax._src.xla_bridge:444: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kchinas/miniconda3/envs/GPU_optimization/lib/python3.10/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 135, in _version_check\n",
      "    version = get_version()\n",
      "RuntimeError: jaxlib/cuda/versions_helpers.cc:81: operation cusparseGetProperty(MAJOR_VERSION, &major) failed: The cuSPARSE library was not found.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kchinas/miniconda3/envs/GPU_optimization/lib/python3.10/site-packages/jax/_src/xla_bridge.py\", line 442, in discover_pjrt_plugins\n",
      "    plugin_module.initialize()\n",
      "  File \"/home/kchinas/miniconda3/envs/GPU_optimization/lib/python3.10/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 230, in initialize\n",
      "    _check_cuda_versions(raise_on_first_error=True)\n",
      "  File \"/home/kchinas/miniconda3/envs/GPU_optimization/lib/python3.10/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 199, in _check_cuda_versions\n",
      "    _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n",
      "  File \"/home/kchinas/miniconda3/envs/GPU_optimization/lib/python3.10/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 139, in _version_check\n",
      "    raise RuntimeError(err_msg) from e\n",
      "RuntimeError: Unable to load cuSPARSE. Is it installed?\n",
      "WARNING:2025-07-08 21:25:14,832:jax._src.xla_bridge:791: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Making sure we are using GPU\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)\n",
    "\n",
    "\"\"\"\n",
    "/tmp/ipykernel_38924/3960750258.py:3: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
    "  print(xla_bridge.get_backend().platform)\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib64/libcusparse.so\n",
      "/usr/local/cuda/lib64/libcusparse.so.12\n",
      "/usr/local/cuda/lib64/libcusparse.so.12.1.3.153\n",
      "/usr/local/cuda/lib64/libcusparse_static.a\n"
     ]
    }
   ],
   "source": [
    "! ls /usr/local/cuda/lib64/libcusparse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can follow the [derivation done by Diego Assencio](https://dassencio.org/33) on the Lagrangian formulation of the double pendulum. From this, one observes that the Lagrangian formualtion of the double pendulum becomes\n",
    "\n",
    "$$\n",
    "    \\mathcal{L} = \\frac{1}{2}\\left( m_1 + m_2 \\right)l^2_1\\dot{\\theta}^2_1 + \\frac{1}{2}m_2l^2_2\\dot{\\theta}^2_2 + m_2l_1l_2\\dot{\\theta}_1\\dot{\\theta}_2\\cos(\\theta_1-\\theta_2) + \\left(m_1+m_2\\right)gl_1\\cos(\\theta_1) + m_2gl_2\\cos(\\theta_2) \n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mathcal{L} = \\frac{1}{2}m_1 l^2_1\\dot{\\theta}^2_1 + \\frac{1}{2}m_2\\left(l^2_1\\dot{\\theta}^2_1 + l^2_2\\dot{\\theta}^2_2 + 2l_1l_2\\dot{\\theta}_1\\dot{\\theta}_2\\cos(\\theta_1-\\theta_2)\\right) + \\left(m_1+m_2\\right)gl_1\\cos(\\theta_1) + m_2gl_2\\cos(\\theta_2) \n",
    "$$\n",
    "\n",
    "Where all the terms that do not contain the gravity $g$ belong to the kinetic energy. To obtain the canonical momenta $p_{\\theta_j}$ associates with the coordinate $\\theta_j$, one just computes $\\partial \\mathcal{L} / \\partial \\dot{\\theta}_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Lagrangian\n",
    "def get_Lagrangian(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    Œ∏_1, Œ∏_2 = q\n",
    "    Œ∏_dot_1, Œ∏_dot_2 = q_dot\n",
    "    \n",
    "    E_kin_1 = 0.5*m1*jnp.square(l1*Œ∏_dot_1)\n",
    "    E_kin_2 = 0.5*m2*( jnp.square(l1*Œ∏_dot_1) + jnp.square(l2*Œ∏_dot_2) + 2*l1*l2*Œ∏_dot_1*Œ∏_dot_2*jnp.cos(Œ∏_1-Œ∏_2) )\n",
    "    \n",
    "    T = E_kin_1 + E_kin_2\n",
    "    V = -g*m2*( l1*jnp.cos(Œ∏_1) +  l2*jnp.cos(Œ∏_2) ) - g*m1*l1*jnp.cos(Œ∏_1)\n",
    "    return T - V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to assess for the quality of the Lagrangian-NN as there's an analytical solution for this problem. By following Diego's derivation, one observes that\n",
    "\n",
    "$$\n",
    "    \\displaystyle\\frac{d}{dt}\n",
    "    \\left( \\begin{matrix} \\theta_1 \\\\[1pt] \\theta_2 \\\\[1pt] \\omega_1 \\\\[1pt] \\omega_1 \\end{matrix} \\right)\n",
    "    =\n",
    "    \\left( \\begin{matrix} \\omega_1 \\\\ \\omega_2 \\\\ g_1(\\theta_1,\\theta_2,\\omega_1,\\omega_2)\n",
    "    \\\\ g_2(\\theta_1,\\theta_2,\\omega_1,\\omega_2) \\end{matrix} \\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    g_1 := \\displaystyle\\frac{f_1 - \\alpha_1 f_2}{1 - \\alpha_1\\alpha_2}\n",
    "    \\quad\\quad\n",
    "    g_2 := \\displaystyle\\frac{f_2 - \\alpha_2 f_1}{1 - \\alpha_1\\alpha_2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    & \\alpha_1(\\theta_1,\\theta_2) ~:=~ \\displaystyle\\frac{l_2}{l_1}\\left(\\frac{m_2}{m_1 + m_2}\\right)\\cos(\\theta_1 - \\theta_2)\\\\\n",
    "    &\\alpha_2(\\theta_1,\\theta_2) ~:=~ \\frac{l_1}{l_2}\\cos(\\theta_1-\\theta_2)\\\\\n",
    "    &\\displaystyle f_1(\\theta_1, \\theta_2, \\dot{\\theta}_1, \\dot{\\theta}_2) ~:=~\n",
    "    -\\frac{l_2}{l_1}\\left(\\frac{m_2}{m_1+m_2}\\right) \\dot{\\theta}_2^2\\sin(\\theta_1 - \\theta_2)\n",
    "    - \\frac{g}{l_1} \\sin\\theta_1 \\\\\n",
    "    &\\displaystyle f_2(\\theta_1, \\theta_2, \\dot{\\theta}_1, \\dot{\\theta}_2) ~:=~\n",
    "    \\frac{l_1}{l_2}\\dot{\\theta}_1^2\\sin(\\theta_1-\\theta_2) - \\frac{g}{l_2} \\sin\\theta_2 \n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "These functions will also be computed to then create the loss function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical_sol(state, t=0, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    Œ∏_1, Œ∏_2, Œ∏_dot_1, Œ∏_dot_2 = state\n",
    "    \n",
    "    Œ±1 = (l2/l1)*(m2/(m1+m2))*jnp.cos(Œ∏_1-Œ∏_2)\n",
    "    Œ±2 = (l1/l2)*jnp.cos(Œ∏_1-Œ∏_2)\n",
    "    \n",
    "    f1 = -(l2/l1)*(m2/(m1+m2))*jnp.square(Œ∏_dot_2)*jnp.sin(Œ∏_1-Œ∏_2) - g*jnp.sin(Œ∏_1)/l1\n",
    "    f2 = (l1/l2)*jnp.square(Œ∏_dot_1)*jnp.sin(Œ∏_1-Œ∏_2) - g*jnp.sin(Œ∏_2)/l2\n",
    "    \n",
    "    g1 = (f1 - Œ±1*f2)/(1 - Œ±1*Œ±2)\n",
    "    g2 = (f2 - Œ±2*f1)/(1 - Œ±1*Œ±2)\n",
    "    \n",
    "    return jnp.stack([Œ∏_dot_1,Œ∏_dot_2,g1,g2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to obtain the dynamics of the system with numerical integration. We take the vectorized form of the Euler-Lagrange equation \n",
    "\n",
    "$$\n",
    "    \\ddot q = (\\nabla_{\\dot q}\\nabla_{\\dot q}^{\\top}\\mathcal{L})^{-1}[\\nabla_q \\mathcal{L} - (\\nabla_{q}\\nabla_{\\dot q}^{\\top}\\mathcal{L})\\dot q]\n",
    "$$\n",
    "\n",
    "It is possible to notice that the neural network will depend on the inverse of a Hessian matrix during the forward pass. This is not an easy implementation; however, the authors of the research propose the usage of JAX, which can implement the previous equation straightforwardly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eqs_of_motion(L, state, t=None):\n",
    "  q, q_t = jnp.split(state,2)\n",
    "  q_tt =  ( jnp.linalg.pinv( jax.hessian(L,1)(q,q_t) ) @ (\n",
    "              jax.grad(L,0)(q,q_t) - jax.jacobian(( jax.jacobian(L,1) ),0)(q,q_t)@q_t\n",
    "            )\n",
    "          )\n",
    "  return jnp.concatenate([q_t, q_tt])\n",
    "\n",
    "# Which is equivalent to:\n",
    "def get_eqs_of_motion__more_simple(L, state, t=None):\n",
    "  q, q_t = jnp.split(state, 2)\n",
    "    \n",
    "  M_inv = jnp.linalg.pinv(jax.hessian(L, 1)(q, q_t))  # ‚àÇ¬≤L/‚àÇqÃá¬≤\n",
    "  f = jax.grad(L, 0)(q, q_t)                          # ‚àÇL/‚àÇq\n",
    "  J = jax.jacobian(jax.grad(L, 1), 0)(q, q_t)         # ‚àÇ/‚àÇq (‚àÇL/‚àÇqÃá)\n",
    "    \n",
    "  q_tt = M_inv @ (f - J @ q_t)\n",
    "  return jnp.concatenate([q_t, q_tt])\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "def JIT_solve_lagrangian(L, initial_state, **kwargs):\n",
    "  #         We currently run odeint on CPUs only, because its cost is \n",
    "  #         dominated by control flow, which is slow on GPUs.\n",
    "  @partial(jax.jit, backend='cpu')\n",
    "  def f(initial_state): # The creation of a pure function.\n",
    "    return odeint(partial(get_eqs_of_motion, L), initial_state, **kwargs)\n",
    "  return f(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of this tutorial made an elegant, yet compact, representation of the equations of motions. Here, I would like to dwelve more into the meaning of each line. \n",
    "\n",
    "**REMARKS** \n",
    "* `jax.jacobian(jax.jacobian(L,0)(q,q_t) @ q_t)`: is elegant but dense. It computes ‚àÇ/‚àÇq of (‚àÇL/‚àÇq @ q_dot), which is part of the standard Euler-Lagrange equation.\n",
    "* `jnp.linalg.pinv`: is good for generality, but can sometimes introduce numerical instability. If the system is known to be well-behaved, `jnp.linalg.solve` may be faster and more accurate (but `pinv` is safer for a generic setup like LNN training).\n",
    "\n",
    "\n",
    "**`get_eqs_of_motion(L, t, state):`** \n",
    "* This second-order differentiation handles the time derivative of the partial w.r.t. velocity ‚Äî that is, it traces how the Lagrangian's \"momentum term\" evolves.\n",
    "* The full derivative of the state is returned. The output is in the form of \n",
    "\n",
    "$$\n",
    "    \\frac{d}{dt} \\begin{bmatrix}\n",
    "           q \\\\\n",
    "           \\dot{q} \\\\\n",
    "         \\end{bmatrix} = \\begin{bmatrix}\n",
    "           \\dot{q} \\\\\n",
    "           \\ddot{q} \\\\\n",
    "         \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is now suitable for any ODE solver like `odeint`.\n",
    "\n",
    "\n",
    "**`solve_lagrangian(L, initial_state, **kwargs):`** this is a wrapper (nested `def`s) that:\n",
    "1. Fixes the Lagrangian `L` for use in `get_eqs_of_motion`.\n",
    "2. JIT-compiles the call to `odeint` (using CPU backend due to control flow complexity).\n",
    "\n",
    "* **Why JIT on CPU?**\n",
    "  * odeint in JAX has complex control flow (e.g., branching based on tolerances, adaptive step sizes). GPUs handle vectorized math well but can be slower at control flow. So you force it to run on CPU for now.\n",
    "  * JAX now has better support for GPU control flow via jax.experimental.ode.odeint, but performance gains are still context-dependent.\n",
    "\n",
    "* **Why use an inner function for `solve_lagrangian` instead of JIT-decorating it directly?** For instance:\n",
    "\n",
    "```Python\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def solve_lagrangian(L, initial_state, **kwargs):\n",
    "    return odeint(partial(get_eqs_of_motion, L), initial_state, **kwargs)\n",
    "```\n",
    "\n",
    "Both versions can work‚Ä¶ BUT there‚Äôs a subtle difference in JAX tracing behavior and Python flexibility.\n",
    "\n",
    "1. **JAX JIT requires static arguments.**\n",
    "  * JAX's `@jit` compiler likes arguments to be **static or immutable** ‚Äî e.g., the function `L` or other Python-side closures.\n",
    "  * When you decorate the whole function `solve_lagrangian(...)` with `@jit`, `L` and `**kwargs` become part of the JAX tracing process.\n",
    "  * This can lead to **recompilation** every time you pass a different `L`, or even worse ‚Äî a `TypeError` if `L` isn't a JAX-friendly object (e.g., a function or closure it can‚Äôt serialize).\n",
    "\n",
    "UNDERSTAND: Using an inner function like this:\n",
    "```Python\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def f(initial_state):\n",
    "```\n",
    "  * fixes `L` and `**kwargs` outside the JIT scope. **That makes `f` a pure function of `initial_state`**, which is safer and more efficient for JAX's tracer.\n",
    "    * Remember this was something mentioned in the KTH school.\n",
    "\n",
    "2. **THIS IS ACTUALLY A CLEANER COMPILATION**\n",
    "\n",
    "With this pattern:\n",
    "\n",
    "```Python\n",
    "def solve_lagrangian(L, initial_state, **kwargs):\n",
    "    @partial(jax.jit, backend='cpu')\n",
    "    def f(initial_state):\n",
    "        ...\n",
    "```\n",
    "one:\n",
    "* Avoid JAX tracing the `L` function directly\n",
    "* **Minimize the number of recompilations when calling `solve_lagrangian` multiple times**\n",
    "* Keep the outer function Pythonic and flexible (e.g., you could swap in different `L`s dynamically)\n",
    "\n",
    "3. **EASIER DEBUGGING**\n",
    "* If you put the `@jit` on the outer function, debugging becomes harder, especially if something goes wrong inside the call to `odeint`. If it's wrapped in `f(...)`, you can:\n",
    "  * Call the inner function un-jitted for debugging\n",
    "  * **Catch errors before JAX compiles everything**\n",
    "\n",
    "4. **REMEMBER**: You want **clean**, JIT-optimized computation **of just the numerical function** (not the logic/structure).\n",
    "* So yep ‚Äî both can work, but the version with the inner function is safer, cleaner, and more efficient for dynamically passed Lagrangians or kwargs.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT IS JIT AND WHY IS IT SO IMPORTANT?\n",
    "\n",
    "JIT stands for Just-In-Time compilation. It means: **Instead of interpreting your code line by line (like Python normally does), JIT compiles the code into optimized machine instructions at runtime, right before it's executed.**\n",
    "\n",
    "In JAX:\n",
    "\n",
    "```Python\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    return x**2 + jnp.sin(x)\n",
    "```\n",
    "\n",
    "When you call f(x), JAX:\n",
    "\n",
    "* Traces the function to record the operations it does (like a computation graph).\n",
    "* Compiles those operations into fast **XLA code** (XLA = Accelerated Linear Algebra compiler).\n",
    "* Caches the compiled version so that next time it just runs the optimized code.\n",
    "\n",
    "Thanks to JIT:\n",
    "\n",
    "* You get C/C++-level speed (or better, especially on GPU).\n",
    "* You avoid Python overhead (e.g., no type checking, no Python object boxing/unboxing).\n",
    "* You can run code efficiently on **CPU**, **GPU**, or **TPU**.\n",
    "\n",
    "**üß© How does this compare to Fortran or C++?**\n",
    "\n",
    "| Feature           | Python (standard) | JAX with JIT                | Fortran / C++         |\n",
    "| ----------------- | ----------------- | --------------------------- | --------------------- |\n",
    "| Language type     | Interpreted       | Interpreted + JIT           | Compiled              |\n",
    "| Compilation step? | ‚ùå (usually none)  | ‚úÖ JIT with XLA              | ‚úÖ Ahead-of-Time  (compiled before execution)       |\n",
    "| Performance       | üê¢ Slow           | üöÄ Near C-level speed       | üöÄ Fast               |\n",
    "| Flexibility       | ‚úÖ Very high       | ‚úÖ High (JIT has limits)     | ‚ùå Less flexible       |\n",
    "| Compilation time  | None              | JIT happens *on first call* | Happens at build time |\n",
    "\n",
    "**üß© What does @partial(...) do?**\n",
    "\n",
    "The @partial decorator comes from Python's functools module and is used to pre-fill some arguments of a function, effectively locking in some parameters ahead of time.\n",
    "\n",
    "In other words, it returns a new version of a function with some arguments already fixed. For instance: \n",
    "\n",
    "```Python\n",
    "from functools import partial\n",
    "\n",
    "def f(a, b, c):\n",
    "    return a + b + c\n",
    "\n",
    "g = partial(f, b=2)\n",
    "g(a=1, c=3)  # returns 1 + 2 + 3 = 6\n",
    "```\n",
    "\n",
    "**üß© Why do we use `@partial` with decorators like `@jax.jit`?**\n",
    "\n",
    "When you decorate a function like this:\n",
    "\n",
    "```Python\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def f(x):\n",
    "    return ...\n",
    "```\n",
    "\n",
    "That‚Äôs exactly the same as:\n",
    "\n",
    "```Python\n",
    "\n",
    "f = partial(jax.jit, backend='cpu')(f)\n",
    "```\n",
    "\n",
    "You're passing `backend='cpu'` as a pre-filled argument to `jax.jit`. So:\n",
    "* `jax.jit` is a function that returns a **decorator**\n",
    "* `partial(jax.jit, backend='cpu')` becomes a customized decorator\n",
    "* `@partial(...)` is just the decorator version of `partial(...)`\n",
    "\n",
    "\n",
    "### Setting up helper functions for the dynamics of the system. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROGRAMMING OPTIMIZATION\n",
    "# Double pendulum dynamics via the rewritten Euler-Lagrange\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def JIT_solve_autograd(initial_state, times, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "  L = partial(get_Lagrangian, m1=m1, m2=m2, l1=l1, l2=l2, g=g)\n",
    "  return JIT_solve_lagrangian(L, initial_state, t=times, rtol=1e-10, atol=1e-10)\n",
    "\n",
    "# Double pendulum dynamics via analytical forces taken from Diego's blog\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def JIT_solve_analytical(initial_state, times):\n",
    "  return odeint(get_analytical_sol, initial_state, t=times, rtol=1e-10, atol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING SOLUTIONS\n",
    "def normalize_dp(state):\n",
    "  # wrap generalized coordinates to [-pi, pi]\n",
    "  return jnp.concatenate([(state[:2] + np.pi) % (2 * np.pi) - np.pi, state[2:]])\n",
    "\n",
    "def rk4_step(f, x, t, h):\n",
    "  # one step of runge-kutta integration\n",
    "  k1 = h * f(x, t)\n",
    "  k2 = h * f(x + k1/2, t + h/2)\n",
    "  k3 = h * f(x + k2/2, t + h/2)\n",
    "  k4 = h * f(x + k3, t + h)\n",
    "  return x + 1/6 * (k1 + 2*k2 + 2*k3 + k4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY normalize the variables?**\n",
    "The angles $\\theta_1$ and $\\theta_2$ in a pendulum are periodic ‚Äî meaning:\n",
    "* $\\theta = \\pi \\rightarrow \\theta = -\\pi$\n",
    "* $\\theta = 2\\pi \\rightarrow \\theta = 0$\n",
    "\n",
    "**But numerically, they are not treated the same** unless you explicitly wrap or normalize them. This helps:\n",
    "* Avoid large angle drift (e.g., 20 radians vs. 1 radian)\n",
    "* Make trajectory comparisons clearer\n",
    "* Most importantly: **Improve numerical stability**\n",
    "\n",
    "### **NEXT:**  Compare Autograd vs. Analytic solutions\n",
    "\n",
    "Now that we have the Euler-Lagrange equation, one can use automatic differentiation to obtain the dynamics of the system; this approach must match the analytic solution.To test this, the authors opt for a random initial state, then the analytic solution is slightly perturbed to assess how sensitive it is to initial conditions. The latter will help in assessing the quality of performance of autograd. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an initial state\n",
    "x0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
    "noise = np.random.RandomState(0).randn(x0.size)\n",
    "t = np.linspace(0, 45, num=100, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dynamics analytically\n",
    "%time x_analytical = jax.device_get(JIT_solve_analytical(x0,t))\n",
    "noise_coeff_1, noise_coeff_2 = 1e-10, 1e-11\n",
    "x_perturbed_1 = jax.device_get(JIT_solve_analytical(x0 + noise_coeff_1*noise, t))\n",
    "x_perturbed_2 = jax.device_get(JIT_solve_analytical(x0 + noise_coeff_2*noise, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dynamics via autograd / the rewritten E-L equation\n",
    "%time x_autograd = jax.device_get(JIT_solve_autograd(x0, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3], dpi=120) #; plt.xlim(0, 100)\n",
    "\n",
    "y_min, y_max = -50, 150\n",
    "x_min, x_max = None, None\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Analytic vs Perturbed ($\\epsilon$={})\".format(noise_coeff_1))\n",
    "plt.xlabel(\"Time\") ; plt.ylabel(\"State\")\n",
    "plt.plot(t, x_analytical[:, 0], 'g-', label='$q$')\n",
    "plt.plot(t, x_analytical[:, 1], 'c-', label='$\\dot q$')\n",
    "plt.plot(t, x_perturbed_1[:, 0], 'g--', label='pert. $q$')\n",
    "plt.plot(t, x_perturbed_1[:, 1], 'c--', label='pert. $\\dot q$')\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.legend(fontsize=6)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Analytic vs Perturbed ($\\epsilon$={})\".format(noise_coeff_2))\n",
    "plt.xlabel(\"Time\") ; plt.ylabel(\"State\")\n",
    "plt.plot(t, x_analytical[:, 0], 'g-', label='$q$')\n",
    "plt.plot(t, x_analytical[:, 1], 'c-', label='$\\dot q$')\n",
    "plt.plot(t, x_perturbed_2[:, 0], 'g--', label='pert. $q$')\n",
    "plt.plot(t, x_perturbed_2[:, 1], 'c--', label='pert. $\\dot q$')\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.legend(fontsize=6)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Analytic vs Autograd\")\n",
    "plt.xlabel(\"Time\") ; plt.ylabel(\"State\")\n",
    "plt.plot(t, x_analytical[:, 0], 'g-', label='$q$')\n",
    "plt.plot(t, x_analytical[:, 1], 'c-', label='$\\dot q$')\n",
    "plt.plot(t, x_autograd[:, 0], 'g--', label='autograd $q$')\n",
    "plt.plot(t, x_autograd[:, 1], 'c--', label='autograd $\\dot q$')\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.legend(fontsize=6)\n",
    "\n",
    "plt.tight_layout() ; plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perturbation comparable to $\\epsilon\\sim1\\cdot10^{-10}$ causes the trajectory to deviate non-linearly at early stages of time. Similarly, the `autograd` solution, also deviates at an early stage of time. Small differences in numerical schemes or tolerances cause divergence over time ‚Äî and that's expected. \n",
    "\n",
    "## GENERATING THE TRAINING DATA FOR THE LNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data with the analytical solution. \n",
    "\n",
    "dt = 0.01\n",
    "N = 1500\n",
    "analytical_step = jax.jit( jax.vmap( partial(rk4_step, get_analytical_sol, t=0.0, h=dt) ) ) # type = jaxlib._jax.PjitFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
    "t = np.arange(N, dtype=np.float32) # time steps 0 to N\n",
    "%time x_train = jax.device_get(JIT_solve_analytical(x0, t)) # dynamics for first N time steps\n",
    "%time xt_train = jax.device_get(jax.vmap(get_analytical_sol)(x_train)) # time derivatives of each state\n",
    "%time y_train = jax.device_get(analytical_step(x_train)) # analytical next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.RandomState(0).randn(x0.size)\n",
    "t_test = np.arange(N, 2*N, dtype=np.float32) # time steps N to 2N\n",
    "%time x_test = jax.device_get(JIT_solve_analytical(x0, t_test)) # dynamics for next N time steps\n",
    "%time xt_test = jax.device_get(jax.vmap(get_analytical_sol)(x_test)) # time derivatives of each state\n",
    "%time y_test = jax.device_get(analytical_step(x_test)) # analytical next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "train_vis = jax.vmap(normalize_dp)(x_train)\n",
    "test_vis = jax.vmap(normalize_dp)(x_test)\n",
    "\n",
    "vel_angle = lambda data:  (np.arctan2(data[:,3], data[:,2]) / np.pi + 1)/2\n",
    "vel_color = lambda vangle: np.stack( [np.zeros_like(vangle), vangle, 1-vangle]).T\n",
    "train_colors = vel_color(vel_angle(train_vis))\n",
    "test_colors = vel_color(vel_angle(test_vis))\n",
    "\n",
    "# plot\n",
    "SCALE = 80 ; WIDTH = 0.006\n",
    "plt.figure(figsize=[8,4], dpi=120)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train data\"); plt.xlabel(r'$\\theta_1$'); plt.ylabel(r'$\\theta_2$')\n",
    "plt.quiver(*train_vis.T, color=train_colors, scale=SCALE, width=WIDTH)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test data\"); plt.xlabel(r'$\\theta_1$'); plt.ylabel(r'$\\theta_2$')\n",
    "plt.quiver(*test_vis.T, color=test_colors, scale=SCALE, width=WIDTH)\n",
    "\n",
    "plt.tight_layout(); plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build the NN and define the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the lagrangian with a parameteric model\n",
    "def learned_lagrangian(params):\n",
    "  def lagrangian(q, q_t):\n",
    "    assert q.shape == (2,)\n",
    "    state = normalize_dp(jnp.concatenate([q, q_t]))\n",
    "    return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n",
    "  return lagrangian\n",
    "\n",
    "# define the loss of the model (MSE between predicted q, \\dot q and targets)\n",
    "@jax.jit\n",
    "def loss(params, batch, time_step=None):\n",
    "  state, targets = batch\n",
    "  if time_step is not None:\n",
    "    f = partial(get_eqs_of_motion, learned_lagrangian(params))\n",
    "    preds = jax.vmap(partial(rk4_step, f, t=0.0, h=time_step))(state)\n",
    "  else:\n",
    "    preds = jax.vmap(partial(get_eqs_of_motion, learned_lagrangian(params)))(state)\n",
    "  return jnp.mean((preds - targets) ** 2)\n",
    "\n",
    "\n",
    "#3.  build the neural network model\n",
    "init_random_params, nn_forward_fn = stax.serial(\n",
    "    stax.Dense(128),\n",
    "    stax.Softplus,\n",
    "    stax.Dense(128),\n",
    "    stax.Softplus,\n",
    "    stax.Dense(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the optimization and the data\n",
    "@jax.jit\n",
    "def update_timestep(i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, jax.grad(loss)(params, batch, time_step), opt_state)\n",
    "\n",
    "@jax.jit\n",
    "def update_derivative(i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, jax.grad(loss)(params, batch, None), opt_state)\n",
    "\n",
    "x_train = jax.device_put(jax.vmap(normalize_dp)(x_train))\n",
    "y_train = jax.device_put(y_train)\n",
    "\n",
    "x_test = jax.device_put(jax.vmap(normalize_dp)(x_test))\n",
    "y_test = jax.device_put(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "_, init_params = init_random_params(rng, (-1, 4))\n",
    "\n",
    "# numbers in comments denote stephan's settings\n",
    "batch_size = 100\n",
    "test_every = 10\n",
    "num_batches = 1500\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# adam w learn rate decay\n",
    "opt_init, opt_update, get_params = optimizers.adam(\n",
    "    lambda t: jnp.select([t < batch_size*(num_batches//3),\n",
    "                          t < batch_size*(2*num_batches//3),\n",
    "                          t > batch_size*(2*num_batches//3)],\n",
    "                         [1e-3, 3e-4, 1e-4]))\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "for iteration in range(batch_size*num_batches + 1):\n",
    "  if iteration % batch_size == 0:\n",
    "    params = get_params(opt_state)\n",
    "    train_loss = loss(params, (x_train, xt_train))\n",
    "    train_losses.append(train_loss)\n",
    "    test_loss = loss(params, (x_test, xt_test))\n",
    "    test_losses.append(test_loss)\n",
    "    if iteration % (batch_size*test_every) == 0:\n",
    "      print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
    "  opt_state = update_derivative(iteration, opt_state, (x_train, xt_train))\n",
    "\n",
    "params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
