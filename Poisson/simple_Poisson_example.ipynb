{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example on Poission Equation. \n",
    "\n",
    "\n",
    "Objective: solve\n",
    "\n",
    "$$\n",
    "    \\frac{d^2u(x)}{dx^2} = f(x)\n",
    "$$\n",
    "\n",
    "With boundary conditions $u(0) = u(1) = 0$. \n",
    "\n",
    "Let's pick a source term: $f(x) = \\pi^2\\sin(\\pi x)$. This means that the equation has an exact solution and it is $u(x) =\\sin(\\pi x)$.\n",
    "\n",
    "A Physics-Informed Neural Network is just a normal NN (e.g., MLP) trained to minimize a loss that enforces the physics — in this case, the Poisson equation.\n",
    "We define a neural net $\\hat{u}_{\\theta}(x)$, and **instead of training it with labeled data, we train it by penalizing it when it violates the differential equation.**\n",
    "\n",
    "To train a PINN, we need to define the loss function. The residual of the PINN is \n",
    "\n",
    "$$\n",
    "    r(x) = \\frac{d^2\\hat{u}_{\\theta}(x)}{dx^2} - f(x)\n",
    "$$\n",
    "\n",
    "**FUNDAMENTAL CONCEPTS**: \n",
    "* AUTOMATIC DIFFERENTIATION (AD).\n",
    "    * Forward and Reverse AD \n",
    "    * Reverse AD vs Backpropagation\n",
    "* DIFFERENTIABLE PROGRAMMING\n",
    "    * For instance: differentiable PIC code. Why even bother to make the PIC code differentiable if it's already fast? \n",
    "        * Once it’s differentiable:\n",
    "            * Optimize initial conditions to reach desired states\n",
    "            * Use gradient-based inverse modeling: “Given a final state, what beam velocity vb caused it?”\n",
    "            * Couple it to a **neural network controller** or PDE and train both\n",
    "            * Use it in physics-informed learning frameworks (like PINNs) with real data\n",
    "        * This moves your simulation from static modeling into trainable, learnable physical systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5838532\n",
      "3.5838532\n"
     ]
    }
   ],
   "source": [
    "# Example of forward and reverse automatic differentiation \n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + jnp.sin(x)\n",
    "\n",
    "# Derivative using forward mode\n",
    "df_dx_fwd = jax.jacfwd(f)\n",
    "# Derivative using reverse mode\n",
    "df_dx_rev = jax.grad(f)\n",
    "\n",
    "x = 2.0\n",
    "print(df_dx_fwd(x))  # Forward-mode derivative\n",
    "print(df_dx_rev(x))  # Reverse-mode derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
